from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.email import EmailOperator
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from datetime import datetime, timedelta

# Default arguments for the DAG
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "email_on_failure": True,
    "email": ["ajay.itc.uk@gmail.com", "rupalirwaykole@gmail.com"],
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

# Define the DAG
with DAG(
    dag_id="spark_data_processing_with_email_and_slack",
    default_args=default_args,
    description="A Spark job using Airflow with email and Slack alerts",
    schedule_interval=timedelta(days=1),
    start_date=datetime(2023, 1, 1),
    catchup=False,
) as dag:

    # Task to run Spark job
    spark_job = BashOperator(
        task_id="run_spark_job",
        bash_command="spark-submit --master local /home/ajayconnect/python/process_data.py /home/ajayconnect/data/input.csv /home/ajayconnect/output",
    )

    # Task to send a success email
    success_email = EmailOperator(
        task_id="send_success_email",
        to=["ajay.itc.uk@gmail.com", "rupalirwaykole@gmail.com"],
        subject="Airflow Notification: Spark Job Success",
        html_content="<p>The Spark job completed successfully!</p>",
    )

    # Slack notification for success
    slack_success_notification = SlackWebhookOperator(
        task_id="slack_success_notification",
        slack_webhook_conn_id="slack_connection",
        message="The Spark job completed successfully!",
        channel="#general"
    )

    # Task to send a failure email (optional as email_on_failure is set)
    failure_email = EmailOperator(
        task_id="send_failure_email",
        to="ajay.itc.uk@gmail.com",
        subject="Airflow Notification: Spark Job Failure",
        html_content="<p>The Spark job failed. Please check the logs for details.</p>",
        trigger_rule="one_failed",
    )

    # Slack notification for failure
    slack_failure_notification = SlackWebhookOperator(
        task_id="slack_failure_notification",
        slack_webhook_conn_id="slack_connection",
        message="The Spark job failed. Please check the logs for details.",
        channel="#general",
        trigger_rule="one_failed",
    )

    # Define task dependencies
    spark_job >> success_email >> slack_success_notification
    spark_job >> failure_email >> slack_failure_notification
